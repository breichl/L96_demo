{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "np.random.seed(14)  # For reproducibility\n",
    "torch.manual_seed(14)  # For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_model import (\n",
    "    L96,\n",
    "    RK2,\n",
    "    RK4,\n",
    "    EulerFwd,\n",
    "    L96_eq1_xdot,\n",
    "    integrate_L96_2t,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 20000\n",
    "Forcing, dt, T = 18, 0.01, 0.01 * time_steps\n",
    "\n",
    "# Create a \"synthetic world\" with K=8 and J=32\n",
    "K = 8\n",
    "J = 32\n",
    "W = L96(K, J, F=Forcing)\n",
    "# Get training data for the neural network.\n",
    "\n",
    "# - Run the true state and output subgrid tendencies (the effect of Y on X is xytrue):\n",
    "Xtrue, _, _ = W.run(dt, T, store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a path\n",
    "PATH = \"networks/network_3_layers_100_epoches.pth\"\n",
    "# Load\n",
    "model = torch.load(PATH)\n",
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in model.keys():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0  # filtering small values\n",
    "gamma = 0.0  # give more weights to positive values\n",
    "\n",
    "## get the weight and bias of the NN\n",
    "def get_weight(modelname):\n",
    "    Ws = []\n",
    "    Bs = []\n",
    "    for (i, name) in enumerate(model.keys()):\n",
    "        if i % 2 == 0:\n",
    "            Ws.append(np.array(model[name]))\n",
    "        else:\n",
    "            Bs.append(np.array(model[name]))\n",
    "    return Ws, Bs  # weights and biases\n",
    "\n",
    "\n",
    "# forward pass to calculate the output of each layer\n",
    "def forward_pass(data, Ws, Bs):\n",
    "    L = len(Ws)\n",
    "    forward = [data] + [None] * L\n",
    "\n",
    "    for l in range(L - 1):\n",
    "        forward[l + 1] = np.maximum(0, Ws[l].dot(forward[l])) + Bs[l]  # ativation ReLu\n",
    "\n",
    "    ## for last layer that does not have activation function\n",
    "\n",
    "    forward[L] = Ws[L - 1].dot(forward[L - 1]) + Bs[L - 1]  # linear last layer\n",
    "    return forward\n",
    "\n",
    "\n",
    "def rho(w, l):\n",
    "    w_intermediate = w + [0.0, 0.0, 0.0, 0.0, 0.0][l] * np.maximum(0, w)\n",
    "    return w_intermediate + gamma * np.maximum(0, w_intermediate)\n",
    "\n",
    "\n",
    "def incr(z, l):\n",
    "    return z + [0.0, 0.0, 0.0, 0.0, 0.0][l] * (z**2).mean() ** 0.5 + 1e-9\n",
    "\n",
    "\n",
    "## backward pass to compute the LRP of each layer. Same rule applied to the first layer (input layer)\n",
    "def onelayer_LRP(W, B, forward, nz, zz):\n",
    "    mask = np.zeros((nz))\n",
    "    mask[zz] = 1\n",
    "    L = len(W)\n",
    "    R = [None] * L + [forward[L] * mask]  # start from last layer Relevance\n",
    "\n",
    "    for l in range(0, L)[::-1]:\n",
    "        w = rho(W[l], l)\n",
    "        b = rho(B[l], l)\n",
    "        z = incr(w.dot(forward[l]) + b + epsilon, l)  # step 1 - forward pass\n",
    "        s = np.array(R[l + 1]) / np.array(z)  # step 2 - element-wise division\n",
    "        c = w.T.dot(s)  # step 3 - backward pass\n",
    "        R[l] = forward[l] * c  # step 4 - element-wise product\n",
    "\n",
    "    return R\n",
    "\n",
    "\n",
    "def LRP_alllayer(data, model):\n",
    "    \"\"\"inputs:\n",
    "        data: for single sample, with the right asix, the shape is (nz,naxis)\n",
    "        model: dictionary of weights and biases\n",
    "    output:\n",
    "        LRP, shape: (nx,L+1) that each of the column consist of L+1 array\n",
    "        Relevance of fisrt layer's pixels\"\"\"\n",
    "    nx = data.shape[0]\n",
    "    ## step 1: get the wieghts\n",
    "    Ws, Bs = get_weight(model)\n",
    "\n",
    "    ## step 2: call the forward pass to get the intermediate layers output\n",
    "    inter_layer = forward_pass(data, Ws, Bs)\n",
    "\n",
    "    ## loop over all z and get the LRP of each layer\n",
    "    R_all = [None] * nx\n",
    "    relevance = np.zeros((nx, nx))\n",
    "    for xx in range(nx):\n",
    "        R_all[xx] = onelayer_LRP(Ws, Bs, inter_layer, nx, xx)\n",
    "        relevance[xx, :] = R_all[xx][0]\n",
    "\n",
    "    return np.stack(R_all), relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_many = []\n",
    "for case in range(200):\n",
    "    inputs = np.copy(Xtrue[case, :])\n",
    "\n",
    "    _, Rs = LRP_alllayer(inputs, model)\n",
    "    R_many.append(Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rstack = np.stack(R_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "vect = np.arange(-5, 5.1, 0.5)\n",
    "pl = ax.contourf(\n",
    "    np.arange(1, 9),\n",
    "    np.arange(1, 9),\n",
    "    np.mean(Rstack, 0),\n",
    "    vect,\n",
    "    cmap=plt.get_cmap(\"seismic\"),\n",
    ")\n",
    "ax.set_ylabel(\"Output layer\", fontsize=12)\n",
    "ax.set_xlabel(\"Input Layer relevance\", fontsize=12)\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "\n",
    "fig.colorbar(pl)\n",
    "fig.savefig(\"LRP-L96.jpeg\", bbox_inches=\"tight\", dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now take gradient - nabla_x(y) - note data was not normalized in this example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network structure in pytorch\n",
    "import torch.nn.functional as FF\n",
    "\n",
    "\n",
    "class Net_ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_ANN, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs, 16 neurons for first hidden layer\n",
    "        self.linear2 = nn.Linear(16, 16)  # 16 neurons for second hidden layer\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "    #         self.lin_drop = nn.Dropout(0.1) #regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = FF.relu(self.linear1(x))\n",
    "        x = FF.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net_ANN()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-2\n",
    "R_many = []\n",
    "std_vec = np.std(Xtrue, axis=0)\n",
    "print(std_vec)\n",
    "for case in range(200):\n",
    "    inputs = np.copy(Xtrue[case, :])\n",
    "    # estimate dy/dx = (y(x+epsilon)-y(x))/epsilon\n",
    "    # TODO need to perturb every components\n",
    "    inputs = torch.tensor(np.single(inputs), requires_grad=False)\n",
    "    pred = model(inputs)\n",
    "    perturb = np.single(np.zeros(np.shape(inputs)))\n",
    "    Rs = np.zeros((len(inputs), len(pred)))\n",
    "    for j in range(len(inputs)):\n",
    "        perturb[j] = epsilon\n",
    "        perturb = perturb  # *std_vec # percent change\n",
    "        Rs[j, :] = (\n",
    "            model(torch.tensor(inputs + perturb)) - model(inputs)\n",
    "        ).detach().numpy() / (np.sum(perturb))\n",
    "    R_many.append(Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rstack = np.stack(R_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "vect = np.arange(-1.5, 1.6, 0.1)\n",
    "pl = ax.contourf(\n",
    "    np.arange(1, 9),\n",
    "    np.arange(1, 9),\n",
    "    np.mean(Rstack, 0),\n",
    "    vect,\n",
    "    cmap=plt.get_cmap(\"seismic\"),\n",
    ")\n",
    "ax.set_ylabel(\"Output layer\", fontsize=12)\n",
    "ax.set_xlabel(\"Input Layer relevance\", fontsize=12)\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "\n",
    "fig.colorbar(pl)\n",
    "fig.savefig(\"LRP-L96-gradient.jpeg\", bbox_inches=\"tight\", dpi=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
