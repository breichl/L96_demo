{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal approximation theorem - NNs can approximate any continuous function.\n",
    "A visual demonstration that neural nets can compute any function:\n",
    "http://neuralnetworksanddeeplearning.com/chap4.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Like any ML algorithm, training a neural netwoek requires minimizing some loss function (for a given structure that maps inputs to outputs). \n",
    "\n",
    "- ### The minimization is done using an algorithm called gradient descent\n",
    "(or a variation called stochastic/minibach gradient descent). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using gradient descent in Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest machine learning algorithm is linear regression. We will code up linear regression from scratch with a twist: We will use gradient descent, which is also how neural networks learn.\n",
    "\n",
    "Most of this lesson is pretty much stolen from Jeremy Howard's fast.ai [lesson zero](https://www.youtube.com/watch?v=ACU-T9L4_lI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### In Linear regression we assume that \n",
    "\n",
    "- ### $y = w_0 + \\sum_i w_i x_i $ \n",
    "\n",
    "We look for the $w$ coefficients that give the 'best' prediction for the output ($y$). The best prediction is defined by minimizing some cost function. For linear regression in machine learning task it is usually the mean square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=\"figs/linear_regression_as_neural_network2.png\", width=500)\n",
    "# Image is taken from https://blog.insightdatascience.com/a-quick-introduction-to-vanilla-neural-networks-b0998c6216a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression from scratch\n",
    "\n",
    "We will learn the parameters a and b of a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from IPython.display import HTML\n",
    "\n",
    "# from fastai.basics import *\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from torch import nn\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WHAT IS PYTORCH? (as defined at https://pytorch.org/) \n",
    "\n",
    "Itâ€™s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- ### A replacement for NumPy to use the power of GPUs\n",
    "- ### A deep learning research platform that provides flexibility and speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chose the true parameters we want to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_real = torch.as_tensor([3.0, 2])\n",
    "w_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some data points x and y which lie on the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = torch.ones(n, 2)\n",
    "x[:, 0].uniform_(\n",
    "    -1.0, 1\n",
    ")  # Under score functions in pytorch means replace the value (update)\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor is a data structure which is a fundamental building block of PyTorch. Tensors are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU\n",
    "and more importantly for us - they can keep track of its gradients.\n",
    "\n",
    "For further reading see [here](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x @ w_real + torch.rand(n)  # @ is a matrix product (similar to matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:, 0], y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.as_tensor([-3.0, -5])\n",
    "# w = nn.Parameter(w); w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could find a way to fit our guess for the coefficients the weights ($w_0$ and $w_1$), we could use the exact same method for very complicated tasks (as image recognition). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written in terms of $w_0$ and $w_1$, our **loss function** is:\n",
    "\n",
    "### $Loss = \\frac{1}{n}\\sum_n (y_t - (w_0 x_0 + w_1 x_1))^2 = \\frac{1}{n}\\sum_n (y_t - (w_0 x_0 + w_1))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = x @ w\n",
    "mse(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:, 0], y)\n",
    "plt.scatter(x[:, 0], y_hat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = nn.Parameter(w)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So far we have specified the *model* (linear regression) and the *evaluation criteria* (or *loss function*). Now we need to handle *optimization*; that is, how do we find the best values for weights ($w_0$, $w_1$)? How do we find the best *fitting* linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know how we have to change w_0 and w_1 to reduce the loss, we compute the derivatives or gradients.\n",
    "\n",
    "### $\\frac{\\partial L}{\\partial w_0} = \\frac{1}{n}\\sum_n -2(y_t - (w_0x_0 + w_1))x$\n",
    "\n",
    "### $\\frac{\\partial L}{\\partial w_1} = \\frac{1}{n}\\sum_n -2(y_t - (w_0x_0 + w_1))$\n",
    "\n",
    "### If we know those we can iteratively take little steps down the gradient to reduce the loss aka **gradient descent**. How big our steps are is determined by the **learning rate**.\n",
    "\n",
    "### $w_0^{new} = w_0^{old}$ - Learning-Rate $*  \\frac{\\partial L}{\\partial w_0}$\n",
    "### $w_1^{new} = w_1^{old}$ - Learning-Rate $*  \\frac{\\partial L}{\\partial w_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update2(iteration):\n",
    "    y_hat = x @ w\n",
    "    loss = mse(y, y_hat)\n",
    "    loss.backward()\n",
    "    # calculate the gradient of a tensor! It is now stored at w.grad\n",
    "    with torch.no_grad():  # To prevent tracking history (and using memory) (code block where we don't need to track the gradients but only modify the values of tensors)\n",
    "        w.sub_(lr * w.grad)\n",
    "        # Under score means inplace. lr is the learning rate. Good learning rate is a key part of Neural Networks.\n",
    "        w.grad.zero_()  # We want to zero the gradient before we are re-evaluate it.\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training RNNs. So, the default action is to accumulate (i.e. sum) the gradients on every loss.backward() call.\n",
    "Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives).\n",
    "\n",
    "explenations about how pytorch calculates the gradients can be found here (and in many other sources) - https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(a, b, x):\n",
    "    return a * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.as_tensor([-2.0, -3])\n",
    "w = nn.Parameter(w)\n",
    "lr = 0.001\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x[:, 0], y)\n",
    "(line,) = ax.plot(\n",
    "    x[:, 0],\n",
    "    lin(w.detach().numpy()[0], w.detach().numpy()[1], x.detach().numpy()[:, 0]),\n",
    "    c=\"firebrick\",\n",
    ")\n",
    "# line, = ax.plot(x[:,0], y, c='firebrick')\n",
    "ax.set_title(\"Loss = 0.00\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    for t in range(100):\n",
    "        l = update2(t)\n",
    "    ax.set_title(\"Loss = %.2f\" % l)\n",
    "    line.set_data(\n",
    "        x.detach().numpy()[:, 0],\n",
    "        lin(w.detach().numpy()[0], w.detach().numpy()[1], x.detach().numpy()[:, 0]),\n",
    "    )\n",
    "    return (line,)\n",
    "\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=70, interval=150, blit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might have some difficulties running this cell without importing certain packages.\n",
    "# might need to install: conda install -c conda-forge ffmpeg\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"figs/Gradient_descent2.png\", width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.as_tensor([-2.0, -3])\n",
    "w = nn.Parameter(w)\n",
    "lr = 0.01\n",
    "for t in range(100):\n",
    "    l = update2(t)\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Deep learning we use a variation of gradient descent called [mini-batch gradient descent](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)\n",
    "- ### Instead of calculating the gradient over the whole training data before changing model weights (coefficients), we take a subset (batches) of our data, and change the values of the weights after we calculated the gradient oves a subset of our data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
