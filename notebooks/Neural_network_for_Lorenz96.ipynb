{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using neural networks for L96 parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from sklearn.metrics import r2_score\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from torch_lr_finder import LRFinder # you might need to install the torch-lr-finder package\n",
    "\n",
    "np.random.seed(14)  # For reproducibility\n",
    "torch.manual_seed(14);  # For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_model import (\n",
    "    L96,\n",
    "    RK2,\n",
    "    RK4,\n",
    "    EulerFwd,\n",
    "    L96_eq1_xdot,\n",
    "    integrate_L96_2t,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lorenz (1996)](https://www.ecmwf.int/en/elibrary/10829-predictability-problem-partly-solved) describes a \"two time-scale\" model in two equations (2 and 3) which are:\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}\n",
    "\\\\\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - X_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GCM classes with and without neural network parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_method = RK4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - a GCM class without any parameterization\n",
    "class GCM_no_param:\n",
    "    def __init__(self, F, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        return L96_eq1_xdot(X, self.F)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - a GCM class including a linear parameterization in rhs of equation for tendency\n",
    "class GCM:\n",
    "    def __init__(self, F, parameterization, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.parameterization = parameterization\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        return L96_eq1_xdot(X, self.F) - self.parameterization(param, X)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - a GCM class including a neural network parameterization in rhs of equation for tendency\n",
    "class GCM_network:\n",
    "    def __init__(self, F, network, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        if self.network.linear1.in_features == 1:\n",
    "            X_torch = torch.from_numpy(X).double()\n",
    "            X_torch = torch.unsqueeze(X_torch, 1)\n",
    "        else:\n",
    "            X_torch = torch.from_numpy(np.expand_dims(X, 0)).double()\n",
    "        return L96_eq1_xdot(X, self.F) + np.squeeze(\n",
    "            self.network(X_torch).data.numpy()\n",
    "        )  # Adding NN parameterization\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 20000\n",
    "Forcing, dt, T = 18, 0.01, 0.01 * time_steps\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "W = L96(8, 32, F=Forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting training data (input output pairs): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data for the neural network.\n",
    "\n",
    "# - Run the true state and output subgrid tendencies (the effect of Y on X is xytrue):\n",
    "Xtrue, _, _, xytrue = W.run(dt, T, store=True, return_coupling=True)\n",
    "gcm_no_param = GCM_no_param(Forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to train and test (validation) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 4000  # number of time steps for validation\n",
    "\n",
    "\n",
    "# train:\n",
    "Xtrue_train = Xtrue[\n",
    "    :-val_size, :\n",
    "]  # Flatten because we first use single input as a sample\n",
    "subgrid_tend_train = xytrue[:-val_size, :]\n",
    "\n",
    "# test:\n",
    "Xtrue_test = Xtrue[-val_size:, :]\n",
    "subgrid_tend_test = xytrue[-val_size:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using data loaders \n",
    "\n",
    "- Dataset and Dataloader classes provide a very convenient way of iterating over a dataset while training your machine learning model.\n",
    "- We need to iterate over the data because it is very slow and memory intensive to hold all the data and to use gradient decent over all the data simultaneously (see more details [here](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) and [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html))\n",
    "\n",
    "<!-- This provides a very convenient way of separating the data preparation part from the training procedure.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data loader\n",
    "\n",
    "# Define our X,Y pairs (state, subgrid tendency) for the linear regression local network.local_torch_dataset = Data.TensorDataset(\n",
    "local_torch_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(np.reshape(Xtrue_train, -1)).double(),\n",
    "    torch.from_numpy(np.reshape(subgrid_tend_train, -1)).double(),\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 1024  # Number of sample in each batch\n",
    "\n",
    "loader_local = Data.DataLoader(\n",
    "    dataset=local_torch_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test dataloader\n",
    "\n",
    "local_torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(np.reshape(Xtrue_test, -1)).double(),\n",
    "    torch.from_numpy(np.reshape(subgrid_tend_test, -1)).double(),\n",
    ")\n",
    "\n",
    "loader_local_test = Data.DataLoader(\n",
    "    dataset=local_torch_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(loader_local)  # iterating over the data to get one batch\n",
    "X_iter, subgrid_tend_iter = dataiter.next()\n",
    "\n",
    "print(X_iter)\n",
    "print(subgrid_tend_iter)\n",
    "\n",
    "fontsize = 20\n",
    "plt.plot(X_iter, subgrid_tend_iter, \".\")\n",
    "plt.xlabel(\"state\", fontsize=fontsize)\n",
    "plt.ylabel(\"subgrid tendency\", fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network structure in pytorch \n",
    "If you want to learn more:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks can have many different structures.\n",
    "- Here we will consider fully connected networks\n",
    "- To undersand fully connected networks, we only need to understand Linear regression (and gradient descent).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img\n",
    "src=\"https://miro.medium.com/max/720/1*VHOUViL8dHGfvxCsswPv-Q.png\" width=400>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we will build a linear regression 'network' and later see how to generalize the linear regression in order to use Fully connected neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a network structure in pytorch (here it is a linear network)\n",
    "class linear_reg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(linear_reg, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 1)  # a single input and a single output\n",
    "\n",
    "    def forward(\n",
    "        self, x\n",
    "    ):  # when calling the model ('linear_reg(input)') it calls automatically the forward method we defined (via __call__ - see https://github.com/pytorch/pytorch/blob/472be69a736c0b2aece4883be9f8b18e2f3dfbbd/torch/nn/modules/module.py#L487)\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_net = linear_reg().double()\n",
    "print(lin_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the nework to get a prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how to plug a sample into the network\n",
    "input1 = torch.randn(1, 1).double()\n",
    "out = lin_net(input1)\n",
    "# when calling the model ('lin_net(input)') it calls automatically the forward method we defined (via __call__ - see https://github.com/pytorch/pytorch/blob/472be69a736c0b2aece4883be9f8b18e2f3dfbbd/torch/nn/modules/module.py#L487)\n",
    "print(\"The output of the random input is:\", out.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To adjust (optimize) the weights we need to define a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()  # MSE loss function\n",
    "X_tmp = next(iter(loader_local))\n",
    "\n",
    "y_tmp = lin_net(torch.unsqueeze(X_tmp[0], 1))  # Predict\n",
    "loss = criterion(y_tmp, torch.unsqueeze(X_tmp[1], 1))  # calculate the MSE loss loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_net.zero_grad()  # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print(\"conv1.bias.grad before backward\")\n",
    "print(lin_net.linear1.bias.grad)\n",
    "\n",
    "loss.backward(\n",
    "    retain_graph=True\n",
    ")  # Computes the gradient of all components current tensor\n",
    "\n",
    "print(\"conv1.bias.grad after backward\")\n",
    "print(lin_net.linear1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the weights using optimizer (basically built in methods for optimization such as SGD, Adam and etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(lin_net.parameters(), lr=0.003, momentum=0.9)\n",
    "print(\"Before backward pass: \\n\", list(lin_net.parameters())[0].data.numpy())\n",
    "loss.backward(retain_graph=True)\n",
    "optimizer.step()\n",
    "print(\"After backward pass: \\n\", list(lin_net.parameters())[0].data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it’s crucial you choose the correct learning rate as otherwise your network will either fail to train, or take much longer to converge. [Here](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d) you can read more about the momentum term in SGD.\n",
    "\n",
    "### The  effective value of the gradient (V) at step t in SGD with momentum ($\\beta$):\n",
    "\\begin{equation}\n",
    "V_t = \\beta V_{t-1} + (1-\\beta) \\nabla_w L(W,X,y)\n",
    "\\end{equation}\n",
    "\n",
    "### and the updates to the weights will be:\n",
    "\\begin{equation}\n",
    "w^{new} = w^{old} - LR * V_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using - Adam (an adaptive learning rate optimization algorithm)\n",
    "\n",
    "The choice of which optimizer we choose might be very important. It will determine how fast the network will be able to learn. Adam is a very popular choice (read more [here](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c) about Adam, and [here](https://ruder.io/optimizing-gradient-descent/index.html#adam) about many types of different optimizers). \n",
    "\n",
    "### Adam is an adaptive learning rate method, which means, it computes individual learning rates for different parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining it all together:  training the whole network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, criterion, trainloader, optimizer):\n",
    "    net.train()\n",
    "    test_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(trainloader):  # for each training step\n",
    "        b_x = Variable(batch_x)  # Inputs\n",
    "        b_y = Variable(batch_y)  # outputs\n",
    "        if (\n",
    "            len(b_x.shape) == 1\n",
    "        ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "            prediction = torch.squeeze(\n",
    "                net(torch.unsqueeze(b_x, 1))\n",
    "            )  # input x and predict based on x\n",
    "        else:\n",
    "            prediction = net(b_x)\n",
    "        loss = criterion(prediction, b_y)  # Calculating loss\n",
    "        optimizer.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients to update weights\n",
    "\n",
    "\n",
    "#         test_loss = test_loss + loss.data.numpy() # Keep track of the loss for convenience\n",
    "#     test_loss /= len(trainloader) # dividing by the number of batches\n",
    "#     print('the loss in this Epoch',test_loss)\n",
    "#     print(b_y.shape)\n",
    "#     print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, criterion, trainloader, optimizer, text=\"validation\"):\n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(\n",
    "            trainloader\n",
    "        ):  # for each training step\n",
    "            b_x = Variable(batch_x)  # Inputs\n",
    "            b_y = Variable(batch_y)  # outputs\n",
    "            if (\n",
    "                len(b_x.shape) == 1\n",
    "            ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "                prediction = torch.squeeze(\n",
    "                    net(torch.unsqueeze(b_x, 1))\n",
    "                )  # input x and predict based on x\n",
    "            else:\n",
    "                prediction = net(b_x)\n",
    "            loss = criterion(prediction, b_y)  # Calculating loss\n",
    "            test_loss = test_loss + loss.data.numpy()  # Keep track of the loss\n",
    "        test_loss /= len(trainloader)  # dividing by the number of batches\n",
    "        #         print(len(trainloader))\n",
    "        print(text + \" loss:\", test_loss)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3  # Number of epocs (the number of times we iterate over the training data during training)\n",
    "optimizer = optim.Adam(\n",
    "    lin_net.parameters(), lr=0.03\n",
    ")  # If we have time we can discuss later the Adam optimizer\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(lin_net, criterion, loader_local, optimizer)\n",
    "    test_model(lin_net, criterion, loader_local, optimizer, \"train\")\n",
    "    test_model(lin_net, criterion, loader_local_test, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lin_net.linear1.weight.data.numpy())\n",
    "print(lin_net.linear1.bias.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds22 = lin_net(\n",
    "    torch.unsqueeze(torch.from_numpy(np.reshape(Xtrue_test[:, 1], -1)).double(), 1)\n",
    ")\n",
    "plt.plot(preds22.detach().numpy()[0:1000], label=\"Predicted values\")\n",
    "plt.plot(subgrid_tend_test[:1000, 1], label=\"True values\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = 10\n",
    "\n",
    "X_full, _, _ = W.run(dt, T_test)  # Full model\n",
    "\n",
    "\n",
    "init_cond = Xtrue[-1, :]\n",
    "\n",
    "gcm_net = GCM_network(Forcing, lin_net)\n",
    "Xnn_1layer, t = gcm_net(init_cond, dt, int(T_test / dt), lin_net)\n",
    "\n",
    "gcm_no_param = GCM_no_param(Forcing)\n",
    "X_no_param, t = gcm_no_param(init_cond, dt, int(T_test / dt))\n",
    "\n",
    "\n",
    "naive_parameterization = lambda param, X: np.polyval(param, X)\n",
    "gcm = GCM(Forcing, naive_parameterization)\n",
    "X_param, t = gcm(init_cond, dt, int(T / dt), param=[0.85439536, 1.75218026])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_i = 200\n",
    "plt.plot(t[:time_i], X_full[:time_i, 4], label=\"Full L96\")\n",
    "plt.plot(t[:time_i], Xnn_1layer[:time_i, 4], \".\", label=\"NN 1 layer\")\n",
    "plt.plot(t[:time_i], X_no_param[:time_i, 4], label=\"no parameterization\")\n",
    "plt.plot(t[:time_i], X_param[:time_i, 4], label=\"linear param (previously used)\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a deeper network  for the Lorenz96  (and using non local features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/publication/319201436/figure/fig1/AS:869115023589376@1584224577926/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values.png\" width=400> *Fig. 1: Visualisation of a two-scale Lorenz '96 system with J = 8 and K = 6. Global-scale values (X k ) are updated based on neighbouring values and a reduction applied to the local-scale values (Y j,k ) associated with that value. Local-scale values are updated based on neighbouring values and the associated global-scale value. The neighbourhood topology of both the local and global-scale values is circular. Image from [Exploiting the chaotic behaviour of atmospheric models with reconfigurable architectures - Scientific Figure on ResearchGate.](https://www.researchgate.net/figure/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values_fig1_319201436)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create non-local train/test data sets (8 inputs, 8 outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non local training data\n",
    "# Define a data loader (8 inputs, 8 outputs)\n",
    "\n",
    "# Define our X,Y pairs (state, subgrid tendency) for the linear regression local network.local_torch_dataset = Data.TensorDataset(\n",
    "torch_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(Xtrue_train).double(),\n",
    "    torch.from_numpy(subgrid_tend_train).double(),\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 1024  # Number of sample in each batch\n",
    "\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test dataloader (8 inputs, 8 outputs)\n",
    "\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(Xtrue_test).double(), torch.from_numpy(subgrid_tend_test).double()\n",
    ")\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a class of a 3 layer fully-connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network structure in pytorch\n",
    "import torch.nn.functional as FF\n",
    "\n",
    "\n",
    "class Net_ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_ANN, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs, 16 neurons for first hidden layer\n",
    "        self.linear2 = nn.Linear(16, 16)  # 16 neurons for second hidden layer\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "    #         self.lin_drop = nn.Dropout(0.1) #regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = FF.relu(self.linear1(x))\n",
    "        x = FF.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function - ReLU (a popular choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If layers would contain only matrix multiplication, everything would be linear: \n",
    "- e.g., 2 layers of weight matrices  A and B (x is the input) would give $A(Bx)$, which is linear (in x)\n",
    "- Therefore we need to introduce some non-linearity (activation function). \n",
    "-The Neural Network with 2 layers of weight matrices  A and B is actually: $A(\\phi(Bx))$ where $\\phi$ is an actication function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLu ativation function is just the max(0,X) - and this what is enabling the NN to be a nonlinear function of the inputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, 50)\n",
    "plt.plot(x, np.maximum(x, 0))\n",
    "plt.title(\"ReLU\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(14)  # For reproducibility\n",
    "nn_3l = Net_ANN().double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50  # Number of epocs\n",
    "optimizer = optim.Adam(nn_3l.parameters(), lr=0.003)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_3l, criterion, loader, optimizer)\n",
    "    train_loss.append(test_model(nn_3l, criterion, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_3l, criterion, loader_test, optimizer))\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds22 = nn_3l(torch.from_numpy(Xtrue_test[:, :]).double())\n",
    "plt.plot(preds22.detach().numpy()[0:1000, 1], label=\"NN Predicted values\")\n",
    "plt.plot(subgrid_tend_test[:1000, 1], label=\"True values\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = 5\n",
    "\n",
    "# X_full,_,_ = W.run(dt, T_test) # Full model\n",
    "\n",
    "gcm_net_3layers = GCM_network(Forcing, nn_3l)\n",
    "Xnn_3layer, t = gcm_net_3layers(init_cond, dt, int(T_test / dt), nn_3l)\n",
    "\n",
    "gcm_net_1layers = GCM_network(Forcing, lin_net)\n",
    "Xnn_1layer, t = gcm_net_1layers(init_cond, dt, int(T_test / dt), lin_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_i = 240\n",
    "channel = 1\n",
    "plt.plot(t[:time_i], X_full[:time_i, channel], label=\"Full L96\")\n",
    "plt.plot(t[:time_i], Xnn_1layer[:time_i, channel], \".\", label=\"NN 1 layer\")\n",
    "plt.plot(t[:time_i], Xnn_3layer[:time_i, channel], \".\", label=\"NN 3 layer\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking over 100 different initial conditions...\n",
    "err1L = list()\n",
    "err3L = list()\n",
    "T_test = 1\n",
    "for i in range(100):\n",
    "    init_cond_temp = Xtrue[i * 10, :]\n",
    "    gcm_net_3layers = GCM_network(Forcing, nn_3l)\n",
    "    Xnn_3layer_tmp, t = gcm_net_3layers(init_cond_temp, dt, int(T_test / dt), nn_3l)\n",
    "\n",
    "    gcm_net_1layers = GCM_network(Forcing, lin_net)\n",
    "    Xnn_1layer_tmp, t = gcm_net_1layers(init_cond_temp, dt, int(T_test / dt), lin_net)\n",
    "\n",
    "    err1L.append(\n",
    "        np.sum(np.abs(Xtrue[i * 10 : i * 10 + T_test * 100 + 1] - Xnn_1layer_tmp))\n",
    "    )\n",
    "    err3L.append(\n",
    "        np.sum(np.abs(Xtrue[i * 10 : i * 10 + T_test * 100 + 1] - Xnn_3layer_tmp))\n",
    "    )\n",
    "print(\"Sum of errors for 1 layer:\", sum(err1L))\n",
    "print(\"Sum of errors for 3 layer:\", sum(err3L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training some more to further improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100  # Number of epocs\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_3l, criterion, loader, optimizer)\n",
    "    train_loss.append(test_model(nn_3l, criterion, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_3l, criterion, loader_test, optimizer))\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save network\n",
    "PATH = \"network_3_layers_100_epoches.pth\"\n",
    "torch.save(nn_3l.state_dict(), PATH)\n",
    "\n",
    "# Load network\n",
    "# path_load = 'network_3_layers_100_epoches.pth'\n",
    "# nn_3l.load_state_dict(torch.load(path_load))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=\"figs/overfitting.png\", width=700)\n",
    "# The figure below is taken from Python Machine Learning book by Sebastian Raschka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization methods are aimed to tackle overfitting. \n",
    "\n",
    "For example, the model on the far right of the plot predicts perfectly on the given set, yet it's not the best choice. Why is that? If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\n",
    "\n",
    "\n",
    "\n",
    "All ML algorithms has some form of regularization. \n",
    "\n",
    "## Useful ways to think of regularization:\n",
    "- Putting constraints on the model\n",
    "aiming to have a better generalizability (avoid modeling the noise or ''remember'' training data). \n",
    "- Adding a term to the loss function so that:  \n",
    "  > Loss = TrainingLoss + Regularization  \n",
    "\n",
    "  puts a penalty for making the model more complex. \n",
    "\n",
    "\n",
    "Very braodly speaking (just to gain intuition) - if we want to reduce the training loss (reduce bias) we should try using a more complex model (if we have enough data) and if we want to reduce overfitting (reduce variace) we should simplify or constraint the model we use (increasing regularization). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropout (added in the definition of the network). \n",
    "- Early stopping\n",
    "- weight decay (added in the optimizer part - see ?optim.Adam)\n",
    "- Data augmentation (usually for images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight decay (L2 norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight decay is usually defined as a term that’s added directly to the update rule.\n",
    "Namely, to update a certain weight $w$ in the $i+1$ iteration, we would use a modified rule:\n",
    "\n",
    "$w_{i+1} = w_{i} - \\gamma ( \\frac{\\partial L}{\\partial w} + A w_{i})$\n",
    "\n",
    "In practice, this is almost identical to L_2 regularization, though there is some difference (e.g., see [here](https://bbabenko.github.io/weight-decay/))\n",
    "\n",
    "Weight decay is one of the parameters of the optimizer - try ?torch.optim.SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a weight decay to a network and try to train it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10  # Number of epocs\n",
    "optimizer = optim.Adam(nn_3l.parameters(), lr=0.003, weight_decay=0.1)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_3l, criterion, loader, optimizer)\n",
    "    train_loss.append(test_model(nn_3l, criterion, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_3l, criterion, loader_test, optimizer))\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By dropping a unit out, we mean temporarily removing it from the network while training, along with all its incoming and outgoing connections. \n",
    "See more details [here](http://jmlr.org/papers/v15/srivastava14a.html).\n",
    "It is usually the most useful regularization that we can do in fully connected layers\n",
    "\n",
    "In convolutional layers dropout makes less sense - see more discussion [here](https://www.kdnuggets.com/2018/09/dropout-convolutional-networks.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image taken from: http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "Image(filename=\"figs/Dropout_layer.png\", width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network structure in pytorch\n",
    "\n",
    "\n",
    "class Net_ANN_dropout(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(Net_ANN_dropout, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.linear3 = nn.Linear(16, 8)\n",
    "        self.drop = nn.Dropout(dropout)  # regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = FF.relu(self.linear1(x))\n",
    "        x = self.drop(x)\n",
    "        x = FF.relu(self.linear2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_3l_drop = Net_ANN_dropout(dropout=0.8).double()  # Exagerated dropout...\n",
    "n_epochs = 10  # Number of epocs\n",
    "optimizer = optim.Adam(nn_3l_drop.parameters(), lr=0.01)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_3l_drop, criterion, loader, optimizer)\n",
    "    train_loss.append(test_model(nn_3l_drop, criterion, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_3l_drop, criterion, loader_test, optimizer))\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to choose a learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visuzlization of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image taken from: https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf\n",
    "Image(filename=\"figs/Loss_function_vis_NN.jpeg\", width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding an optimal learning rate\n",
    "\n",
    "To use the `LRFinder` package, uncomment the import of LRFinder in the top cell and then uncomment the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_3l_LR = Net_ANN().double()\n",
    "# optimizer = optim.Adam(nn_3l_LR.parameters(), lr=1e-7)\n",
    "# lr_finder = LRFinder(nn_3l_LR, optimizer, criterion)\n",
    "# lr_finder.range_test(loader, end_lr=100, num_iter=200)\n",
    "# lr_finder.plot()  # to inspect the loss-learning rate graph\n",
    "# lr_finder.reset()  # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 20  # Number of epocs\n",
    "# optimizer = optim.Adam(nn_3l_LR.parameters(), lr=0.01)\n",
    "# validation_loss = list()\n",
    "# train_loss = list()\n",
    "# # time0 = time()\n",
    "# for epoch in range(1, n_epochs + 1):\n",
    "#     train_model(nn_3l_LR, criterion, loader, optimizer)\n",
    "#     train_loss.append(test_model(nn_3l_LR, criterion, loader, optimizer, \"train\"))\n",
    "#     validation_loss.append(test_model(nn_3l_LR, criterion, loader_test, optimizer))\n",
    "# plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "# plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We converged much faster than before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(nn_3l_LR.parameters(), lr=1e-7)\n",
    "# lr_finder = LRFinder(nn_3l_LR, optimizer, criterion)\n",
    "# lr_finder.range_test(loader, end_lr=100, num_iter=200)\n",
    "# lr_finder.plot()  # to inspect the loss-learning rate graph\n",
    "# lr_finder.reset()  # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 10  # Number of epocs\n",
    "# optimizer = optim.Adam(nn_3l_LR.parameters(), lr=0.001)\n",
    "# validation_loss = list()\n",
    "# train_loss = list()\n",
    "# # time0 = time()\n",
    "# for epoch in range(1, n_epochs + 1):\n",
    "#     train_model(nn_3l_LR, criterion, loader, optimizer)\n",
    "#     train_loss.append(test_model(nn_3l_LR, criterion, loader, optimizer, \"train\"))\n",
    "#     validation_loss.append(test_model(nn_3l_LR, criterion, loader_test, optimizer))\n",
    "# plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "# plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I won't talk about but I recommend reading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNormalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the activation values such that the hidden representation doesn’t vary drastically and also helps us to get improvement in the training speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclic learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To understand cyclic learning rates and the One cycle policy - read more [here](https://sgugger.github.io/the-1cycle-policy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image taken from - https://docs.fast.ai/callbacks.one_cycle.html\n",
    "Image(filename=\"figs/onecycle_params.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use cyclic learning rates: ?optim.lr_scheduler.CyclicLR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
